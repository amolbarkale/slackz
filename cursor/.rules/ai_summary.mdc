---
description: 
globs: 
alwaysApply: true
---
# AI Thread/Channel Summary Feature - PRD

## ðŸŽ¯ **Feature Overview**

**Feature Name**: AI-Powered Thread & Channel Summarizer  
**Implementation**: Local llama3.2:3b via Ollama + Convex Backend  
**Timeline**: 6 hours development  
**Priority**: High (MVP Core Feature)

---

## ðŸ“‹ **Product Requirements**

### **Core Functionality**
1. **Thread Summary**: Generate concise summaries of message threads
2. **Channel Summary**: Summarize entire channel conversations
3. **Context-Aware**: Include participant names, timestamps, and key decisions
4. **Workspace Isolation**: Summaries are workspace-specific
5. **Real-time Generation**: On-demand summary creation

### **User Stories**
- **As a team member**, I want to quickly understand what happened in a long thread without reading every message
- **As a project manager**, I want to generate meeting notes from channel discussions
- **As a new team member**, I want to catch up on channel history efficiently
- **As a busy executive**, I want key decisions and action items highlighted

---

## ðŸ—ï¸ **Technical Architecture**

### **Technology Stack**
```
Frontend: Next.js + React (existing)
Backend: Convex (existing)
AI Engine: llama3.2:3b via Ollama (local)
API Layer: Convex Actions â†’ HTTP â†’ Ollama Server
```

### **Data Flow**
```
User Click â†’ Convex Action â†’ Message Aggregation â†’ Context Building â†’ 
llama3.2:3b Processing â†’ Structured Summary â†’ UI Display
```

### **Local Development Setup**
```bash
# Ollama Installation
curl -fsSL https://ollama.ai/install.sh | sh
ollama pull llama3.2:3b
ollama serve --host localhost --port 11434
```

---

## ðŸŽ¨ **User Interface Design**

### **Thread Summary**
- **Trigger**: "ðŸ“ Generate Summary" button in thread header
- **Location**: Thread sidebar, next to close button
- **Loading State**: Spinner with "Generating summary..." text
- **Output**: Modal with formatted summary

### **Channel Summary**
- **Trigger**: "ðŸ“Š Summarize Channel" button in channel header
- **Options**: Last 24 hours, Last week, All messages
- **Loading State**: Progress indicator with message count
- **Output**: Expandable summary panel

### **Summary Format**
```
ðŸ“ **Thread Summary** (Generated by AI)
**Participants**: Alice, Bob, Charlie
**Duration**: 2 hours (2:30 PM - 4:30 PM)

**Key Discussion Points**:
â€¢ Project timeline discussion
â€¢ Budget allocation decisions
â€¢ Resource assignment

**Decisions Made**:
â€¢ Launch date set for Dec 15th
â€¢ Alice assigned as lead designer
â€¢ Budget approved: $50K

**Action Items**:
â€¢ Bob: Create project timeline by Friday
â€¢ Charlie: Review budget proposal
â€¢ Alice: Start UI mockups next week

**Next Steps**:
â€¢ Team meeting scheduled for Monday
â€¢ Weekly check-ins every Friday
```

---

## ðŸ”§ **Technical Implementation**

### **Database Schema Extensions**
```typescript
// Add to existing schema.ts
summaries: defineTable({
  workspaceId: v.id("workspaces"),
  contextType: v.union(v.literal("thread"), v.literal("channel")),
  contextId: v.string(), // threadId or channelId
  summary: v.string(),
  messageCount: v.number(),
  participantCount: v.number(),
  generatedBy: v.id("members"),
  generatedAt: v.number(),
})
  .index("by_workspace_id", ["workspaceId"])
  .index("by_context", ["workspaceId", "contextType", "contextId"]),
```

### **Convex Actions**
```typescript
// convex/aiSummary.ts
export const generateThreadSummary = action({
  args: {
    workspaceId: v.id("workspaces"),
    channelId: v.optional(v.id("channels")),
    threadId: v.optional(v.id("messages")),
    timeRange: v.optional(v.string()), // "24h", "week", "all"
  },
  handler: async (ctx, args) => {
    // 1. Aggregate messages
    // 2. Build context with user names
    // 3. Call llama3.2:3b via HTTP
    // 4. Parse and format response
    // 5. Cache summary in database
    // 6. Return structured summary
  }
});
```

### **llama3.2:3b Integration**
```typescript
// Ollama API Call
const response = await fetch('http://localhost:11434/api/generate', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({
    model: 'llama3.2:3b',
    prompt: buildSummaryPrompt(messages),
    stream: false,
    options: {
      temperature: 0.3, // More focused responses
      top_p: 0.9,
      max_tokens: 500
    }
  })
});
```

---

## ðŸ“Š **Performance Requirements**

### **Response Times**
- **Thread Summary** (< 50 messages): < 10 seconds
- **Channel Summary** (< 200 messages): < 30 seconds
- **Large Channel** (> 500 messages): < 60 seconds

### **Resource Usage**
- **llama3.2:3b RAM**: 4-6GB maximum
- **CPU Usage**: < 80% during generation
- **Disk Space**: 1GB for model storage

### **Scalability**
- **Concurrent Requests**: 2-3 simultaneous summaries
- **Daily Usage**: 50-100 summaries per workspace
- **Message Limit**: 1000 messages per summary

---

## ðŸ›¡ï¸ **Error Handling & Edge Cases**

### **Error Scenarios**
1. **Ollama Server Down**: Show "AI service unavailable" message
2. **Empty Thread**: "No messages to summarize"
3. **Too Many Messages**: Truncate to last 500 messages
4. **Network Timeout**: Retry once, then show error
5. **Malformed Response**: Use fallback summary template

### **Fallback Behavior**
```typescript
// Fallback summary when AI fails
const fallbackSummary = {
  participants: extractParticipants(messages),
  messageCount: messages.length,
  timeRange: getTimeRange(messages),
  keyTopics: extractKeywords(messages),
  summary: "Summary generation failed. Please try again."
};
```

---

## ðŸ§ª **Testing Strategy**

### **Unit Tests**
- Message aggregation functions
- Context building logic
- Summary parsing and validation
- Error handling scenarios

---

## ðŸš€ **Implementation Phases**

### **Phase 1: Backend Foundation (2 hours)**
- Set up Ollama with llama3.2:3b
- Create message aggregation functions
- Implement basic HTTP integration
- Add database schema for summaries

### **Phase 2: AI Integration (2 hours)**
- Build llama3.2:3b prompt templates
- Implement summary generation logic
- Add response parsing and validation
- Create error handling and fallbacks

### **Phase 3: Frontend UI (1.5 hours)**
- Add summary buttons to thread/channel headers
- Create summary display modal
- Implement loading states and progress
- Add copy-to-clipboard functionality

### **Phase 4: Testing & Polish (0.5 hours)**
- Test with various message lengths
- Validate summary quality
- Fix UI/UX issues
- Add final error handling

---

## ðŸŽ¯ **Definition of Done**

### **MVP Completion Criteria**
- âœ… llama3.2:3b running locally via Ollama
- âœ… Thread summary generation working
- âœ… Channel summary generation working
- âœ… UI buttons and modals implemented
- âœ… Error handling and fallbacks working
- âœ… Basic testing completed
- âœ… Documentation updated

### **Quality Gates**
- All unit tests passing
- Integration tests with Ollama working
- UI responsive on desktop and mobile
- Error scenarios handled gracefully
- Performance within specified limits
- Code reviewed and documented


