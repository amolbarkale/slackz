---
description: 
globs: 
alwaysApply: true
---
# AI Thread/Channel Summary Feature - PRD

## 🎯 **Feature Overview**

**Feature Name**: AI-Powered Thread & Channel Summarizer  
**Implementation**: Local llama3.2:3b via Ollama + Convex Backend  
**Timeline**: 6 hours development  
**Priority**: High (MVP Core Feature)

---

## 📋 **Product Requirements**

### **Core Functionality**
1. **Thread Summary**: Generate concise summaries of message threads
2. **Channel Summary**: Summarize entire channel conversations
3. **Context-Aware**: Include participant names, timestamps, and key decisions
4. **Workspace Isolation**: Summaries are workspace-specific
5. **Real-time Generation**: On-demand summary creation

### **User Stories**
- **As a team member**, I want to quickly understand what happened in a long thread without reading every message
- **As a project manager**, I want to generate meeting notes from channel discussions
- **As a new team member**, I want to catch up on channel history efficiently
- **As a busy executive**, I want key decisions and action items highlighted

---

## 🏗️ **Technical Architecture**

### **Technology Stack**
```
Frontend: Next.js + React (existing)
Backend: Convex (existing)
AI Engine: llama3.2:3b via Ollama (local)
API Layer: Convex Actions → HTTP → Ollama Server
```

### **Data Flow**
```
User Click → Convex Action → Message Aggregation → Context Building → 
llama3.2:3b Processing → Structured Summary → UI Display
```

### **Local Development Setup**
```bash
# Ollama Installation
curl -fsSL https://ollama.ai/install.sh | sh
ollama pull llama3.2:3b
ollama serve --host localhost --port 11434
```

---

## 🎨 **User Interface Design**

### **Thread Summary**
- **Trigger**: "📝 Generate Summary" button in thread header
- **Location**: Thread sidebar, next to close button
- **Loading State**: Spinner with "Generating summary..." text
- **Output**: Modal with formatted summary

### **Channel Summary**
- **Trigger**: "📊 Summarize Channel" button in channel header
- **Options**: Last 24 hours, Last week, All messages
- **Loading State**: Progress indicator with message count
- **Output**: Expandable summary panel

### **Summary Format**
```
📝 **Thread Summary** (Generated by AI)
**Participants**: Alice, Bob, Charlie
**Duration**: 2 hours (2:30 PM - 4:30 PM)

**Key Discussion Points**:
• Project timeline discussion
• Budget allocation decisions
• Resource assignment

**Decisions Made**:
• Launch date set for Dec 15th
• Alice assigned as lead designer
• Budget approved: $50K

**Action Items**:
• Bob: Create project timeline by Friday
• Charlie: Review budget proposal
• Alice: Start UI mockups next week

**Next Steps**:
• Team meeting scheduled for Monday
• Weekly check-ins every Friday
```

---

## 🔧 **Technical Implementation**

### **Database Schema Extensions**
```typescript
// Add to existing schema.ts
summaries: defineTable({
  workspaceId: v.id("workspaces"),
  contextType: v.union(v.literal("thread"), v.literal("channel")),
  contextId: v.string(), // threadId or channelId
  summary: v.string(),
  messageCount: v.number(),
  participantCount: v.number(),
  generatedBy: v.id("members"),
  generatedAt: v.number(),
})
  .index("by_workspace_id", ["workspaceId"])
  .index("by_context", ["workspaceId", "contextType", "contextId"]),
```

### **Convex Actions**
```typescript
// convex/aiSummary.ts
export const generateThreadSummary = action({
  args: {
    workspaceId: v.id("workspaces"),
    channelId: v.optional(v.id("channels")),
    threadId: v.optional(v.id("messages")),
    timeRange: v.optional(v.string()), // "24h", "week", "all"
  },
  handler: async (ctx, args) => {
    // 1. Aggregate messages
    // 2. Build context with user names
    // 3. Call llama3.2:3b via HTTP
    // 4. Parse and format response
    // 5. Cache summary in database
    // 6. Return structured summary
  }
});
```

### **llama3.2:3b Integration**
```typescript
// Ollama API Call
const response = await fetch('http://localhost:11434/api/generate', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({
    model: 'llama3.2:3b',
    prompt: buildSummaryPrompt(messages),
    stream: false,
    options: {
      temperature: 0.3, // More focused responses
      top_p: 0.9,
      max_tokens: 500
    }
  })
});
```

---

## 📊 **Performance Requirements**

### **Response Times**
- **Thread Summary** (< 50 messages): < 10 seconds
- **Channel Summary** (< 200 messages): < 30 seconds
- **Large Channel** (> 500 messages): < 60 seconds

### **Resource Usage**
- **llama3.2:3b RAM**: 4-6GB maximum
- **CPU Usage**: < 80% during generation
- **Disk Space**: 1GB for model storage

### **Scalability**
- **Concurrent Requests**: 2-3 simultaneous summaries
- **Daily Usage**: 50-100 summaries per workspace
- **Message Limit**: 1000 messages per summary

---

## 🛡️ **Error Handling & Edge Cases**

### **Error Scenarios**
1. **Ollama Server Down**: Show "AI service unavailable" message
2. **Empty Thread**: "No messages to summarize"
3. **Too Many Messages**: Truncate to last 500 messages
4. **Network Timeout**: Retry once, then show error
5. **Malformed Response**: Use fallback summary template

### **Fallback Behavior**
```typescript
// Fallback summary when AI fails
const fallbackSummary = {
  participants: extractParticipants(messages),
  messageCount: messages.length,
  timeRange: getTimeRange(messages),
  keyTopics: extractKeywords(messages),
  summary: "Summary generation failed. Please try again."
};
```

---

## 🧪 **Testing Strategy**

### **Unit Tests**
- Message aggregation functions
- Context building logic
- Summary parsing and validation
- Error handling scenarios

---

## 🚀 **Implementation Phases**

### **Phase 1: Backend Foundation (2 hours)**
- Set up Ollama with llama3.2:3b
- Create message aggregation functions
- Implement basic HTTP integration
- Add database schema for summaries

### **Phase 2: AI Integration (2 hours)**
- Build llama3.2:3b prompt templates
- Implement summary generation logic
- Add response parsing and validation
- Create error handling and fallbacks

### **Phase 3: Frontend UI (1.5 hours)**
- Add summary buttons to thread/channel headers
- Create summary display modal
- Implement loading states and progress
- Add copy-to-clipboard functionality

### **Phase 4: Testing & Polish (0.5 hours)**
- Test with various message lengths
- Validate summary quality
- Fix UI/UX issues
- Add final error handling

---

## 🎯 **Definition of Done**

### **MVP Completion Criteria**
- ✅ llama3.2:3b running locally via Ollama
- ✅ Thread summary generation working
- ✅ Channel summary generation working
- ✅ UI buttons and modals implemented
- ✅ Error handling and fallbacks working
- ✅ Basic testing completed
- ✅ Documentation updated

### **Quality Gates**
- All unit tests passing
- Integration tests with Ollama working
- UI responsive on desktop and mobile
- Error scenarios handled gracefully
- Performance within specified limits
- Code reviewed and documented


